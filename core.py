from sentence_transformers import SentenceTransformer, util
import torch
import re

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∞—à–∏ –º–æ–¥—É–ª–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
from terms import terms as _base_terms
from detailed_terms import detailed_terms as _base_detailed
from term_aliases import term_aliases as _base_aliases

# –°–ª–æ–≤–∞—Ä—å —Ç—Ä—ë—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: –∞–Ω–≥–ª ‚Üí (–ø–µ—Ä–µ–≤–æ–¥, —Ç—Ä–∞–Ω—Å–ª–∏—Ç)
LANG_VARIANTS = {
    "smart money": ("—É–º–Ω—ã–µ –¥–µ–Ω—å–≥–∏", "—Å–º–∞—Ä—Ç –º–∞–Ω–∏"),
    "order block": ("–±–ª–æ–∫ –æ—Ä–¥–µ—Ä–æ–≤", "–æ—Ä–¥–µ—Ä –±–ª–æ–∫"),
    "market shift": ("—Å–º–µ–Ω–∞ —Ä—ã–Ω–∫–∞", "–º–∞—Ä–∫–µ—Ç —à–∏—Ñ—Ç"),
    # –¥–æ–±–∞–≤—å—Ç–µ —Å—é–¥–∞ –¥—Ä—É–≥–∏–µ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –∫–ª—é—á–∏
}

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑
model = SentenceTransformer("all-MiniLM-L6-v2")

# –°—Ç–æ–ø-—Ñ—Ä–∞–∑—ã –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
STOP_WORDS = [
    "—á—Ç–æ —Ç–∞–∫–æ–µ", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ—è—Å–Ω–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏",
    "–¥–∞–π", "–ø–æ—á–µ–º—É", "–∫–∞–∫", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–ø–æ—è—Å–Ω–∏ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞"
]

def normalize_text(text: str) -> str:
    """
    –ù–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, '_'‚Üí' ', —É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã.
    """
    text = text.lower().replace("_", " ")
    text = re.sub(r'[^\w\s]', '', text)
    for stop in STOP_WORDS:
        if text.startswith(stop):
            text = text[len(stop):].strip()
    return re.sub(r'\s+', ' ', text).strip()

# 1) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
terms = {normalize_text(k): v for k, v in _base_terms.items()}
detailed_terms = {normalize_text(k): v for k, v in _base_detailed.items()}

# 2) –°–±–æ—Ä –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å —É—á—ë—Ç–æ–º LANG_VARIANTS
aliases_clean = {}
for eng_key, v in _base_aliases.items():
    norm_key = normalize_text(eng_key)

    # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
    if isinstance(v, tuple) and len(v) == 2 and isinstance(v[1], list):
        base_aliases = v[1]
    else:
        base_aliases = v if isinstance(v, list) else []

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏ —Ç—Ä–∞–Ω—Å–ª–∏—Ç
    if eng_key in LANG_VARIANTS:
        rus, trans = LANG_VARIANTS[eng_key]
        base_aliases += [rus, trans]

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å—ë –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    aliases_clean[norm_key] = [normalize_text(a) for a in base_aliases]

term_aliases = aliases_clean

# 3) –ü—Ä–µ–¥—Ä–∞—Å—á—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
keys = list(terms.keys())
key_embeddings = model.encode(keys, convert_to_tensor=True)

import re

def get_best_answer(query: str, detailed: bool = False):
    q_orig = normalize_text(query)

    # –ò—â–µ–º –∫–ª—é—á–µ–≤–æ–π —Ç–µ—Ä–º–∏–Ω, —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫—É (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏–ª–∏ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ)
    for term_key in keys:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ term_key –µ—Å—Ç—å –≤ q_orig –∏ –æ–∫—Ä—É–∂—ë–Ω –ª–∏–±–æ –ø—Ä–æ–±–µ–ª–∞–º–∏, –ª–∏–±–æ —Å—Ç–æ–∏—Ç –≤ –Ω–∞—á–∞–ª–µ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
        pos = q_orig.find(term_key)
        if pos != -1:
            before = pos == 0 or q_orig[pos-1] == " "
            after = pos + len(term_key) == len(q_orig) or q_orig[pos+len(term_key)] == " "
            if before and after:
                q = term_key
                break
    else:
        q = q_orig

    # –î–∞–ª–µ–µ –ª–æ–≥–∏–∫–∞ –∫–∞–∫ —Ä–∞–Ω—å—à–µ
    if q in terms:
        ans = detailed_terms.get(q) if detailed else terms[q]
        if not ans:
            ans = terms[q] + "\n\nüìå –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–∫–∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ."
        return ans, "FXPO Squad"

    for key, aliases in term_aliases.items():
        if q in aliases:
            ans = detailed_terms.get(key) if detailed else terms.get(key)
            if not ans:
                fallback = terms.get(key)
                if fallback:
                    ans = fallback + "\n\nüìå –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–∫–∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ."
                else:
                    continue
            return ans, "FXPO Squad"

    query_emb = model.encode(q, convert_to_tensor=True)
    sims = util.cos_sim(query_emb, key_embeddings)[0]
    best_idx = torch.argmax(sims).item()
    best_score = sims[best_idx].item()

    if best_score < 0.5:
        top_idxs = torch.topk(sims, k=3)[1]
        suggestions = [keys[i] for i in top_idxs if sims[i] > 0.3]
        if suggestions:
            return "‚ùì –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: " + ", ".join(suggestions), "FXPO Squad"
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç.", "FXPO Squad"

    best_key = keys[best_idx]
    ans = detailed_terms.get(best_key) if detailed else terms[best_key]
    if not ans:
        ans = terms[best_key] + "\n\nüìå –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–∫–∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ."
    return ans, "FXPO Squad"
